system {
    spark {
        user = ${system.default.user}
        group = ${system.default.group}
        path {
            # uncomment the following section if you want to extract an archive on every run
            archive = {
                 src = ${app.path.systems}"/spark-1.0.0-bin-hadoop1.tgz"
                 dst = ${app.path.systems}
            }
            home    = ${app.path.systems}"/spark-1.0.0-bin-hadoop1"
            config  = ${system.spark.path.home}"/conf"
            log     = ${system.spark.path.home}"/logs"
        }
        config {
            # put list of slaves
            slaves      = ${system.default.config.slaves}
            # spark-env.sh entries
            env {
                HADOOP_CONF_DIR = ${system.hadoop.path.config}
            }
            defaults {
                spark.executor.memory   = "512m"
            }

        }
    }
}
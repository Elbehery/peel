system {
    hadoop {
        user = ${system.default.user}
        group = ${system.default.group}
        path {
            # uncomment the following section if you want to extract an archive on every run
            # archive = {
            #     src = ${app.path.systems}"/hadoop-VERSION.tar.gz"
            #     dst = ${app.path.systems}
            # }
            home = ${app.path.systems}"/hadoop"
            config = ${system.hadoop.path.home}"/conf"
            log = ${system.hadoop.path.home}"/logs"
            input = ${system.hadoop.config.core.fs.default.name}"/tmp/input"
            output = ${system.hadoop.config.core.fs.default.name}"/tmp/output"
        }
        format = true
        config {
            # put list of masters
            masters = ${system.default.config.masters}
            # put list of slaves
            slaves = ${system.default.config.slaves}
            # hadoop-env.sh entries
            env {
                JAVA_HOME = ${system.default.config.java.home}
            }
            # core-site.xml entries
            core {
                fs.default.name = "hdfs://localhost:9000"
            }
            # hdfs-site.xml entries
            hdfs {
                dfs.replication = "1"
                dfs.name.dir = "/tmp/hdfs/name"
                dfs.data.dir = "/tmp/hdfs/data"
            }
            # mapred-site.xml entries
            mapred {
                mapred.job.tracker._root_ = "localhost:9001"
                mapred.tasktracker.map.tasks.maximum = 2
                mapred.tasktracker.reduce.tasks.maximum = 2
            }
        }
    }
}
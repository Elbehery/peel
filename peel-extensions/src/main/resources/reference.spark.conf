system {
    spark {
        user = ${system.default.user}
        group = ${system.default.group}
        path {
            # uncomment the following section if you want to extract an archive on every run
            archive = {
                 src = ${app.path.systems}"/spark-1.0.0-bin-hadoop1.tgz"
                 dst = ${app.path.systems}
            }
            home    = ${app.path.systems}"/spark-1.0.0-bin-hadoop1"
            config  = ${system.spark.path.home}"/conf"
            log     = ${system.spark.config.defaults.spark.eventLog.dir}
        }
        startup {
            max.attempts = ${system.default.startup.max.attempts}
            polling {
                counter = ${system.default.startup.polling.counter}
                interval = ${system.default.startup.polling.interval}
            }
        }
        config {
            # put list of slaves
            slaves      = ${system.default.config.slaves}
            # spark-env.sh entries
            env {
                JAVA_HOME       = ${system.default.config.java.home}
            }
            defaults {
                spark.master            = "local"
                spark.eventLog.enabled  = "true"
                spark.eventLog.dir      = "file://"${system.spark.path.home}"/logs"
            }

        }
    }
}